<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"/> <meta content="width=device-width, initial-scale=1.0" name="viewport"/> <title>JavaScript - Got Detected</title> <meta content="JavaScript Home / JavaScript On This PageKey Challenges in JavaSc..." name="description"/> <meta content="javascript" name="keywords"/> <meta content="index, follow" name="robots"/> <link href="../assets/style.css" rel="stylesheet"/> <!-- Prism.js for syntax highlighting --> <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet"/> <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script> <!-- Fuse.js for search --> <script src="https://cdn.jsdelivr.net/npm/fuse.js@7.0.0/dist/fuse.min.js"></script> </head> <body> <nav class="site-nav"> <a class="brand" href="../index.html">Got Detected</a> <div class="nav-links"> <a href="../index.html">Home</a> <a href="../overview.html">Overview</a> <a href="../concepts/index.html">Concepts</a> <a href="../guides/index.html">Guides</a> <a href="../glossary.html">Glossary</a> </div> <div class="search-container"> <input class="search-input" id="search-input" placeholder="Search..." type="text"/> <div class="search-results" id="search-results"></div> </div> </nav> <main class="content-wrapper"> <h1 id="javascript">JavaScript</h1> <nav class="breadcrumb"> <a href="../index.html">Home</a> / JavaScript </nav> <div class="content-wrapper"> <article> <div class="toc"> <h3 id="on-this-page">On This Page</h3> <ul class="toc-list"> <li class="toc-section"> <a href="#key-challenges-in-javascript-web-scraping">Key Challenges in JavaScript Web Scraping</a> <ul class="toc-subsections"> <li class="toc-subsection"> <a href="#1-handling-dynamic-content">1. Handling Dynamic Content</a> </li> <li class="toc-subsection"> <a href="#2-captcha-solving">2. Captcha Solving</a> </li> <li class="toc-subsection"> <a href="#3-proxies-and-ip-rotation">3. Proxies and IP Rotation</a> </li> </ul> </li> <li class="toc-section"> <a href="#proven-solutions-for-javascript-web-scraping">Proven Solutions for JavaScript Web Scraping</a> <ul class="toc-subsections"> <li class="toc-subsection"> <a href="#1-using-javascript-libraries-and-frameworks">1. Using JavaScript Libraries and Frameworks</a> </li> <li class="toc-subsection"> <a href="#2-captcha-solving-services">2. Captcha Solving Services</a> </li> <li class="toc-subsection"> <a href="#3-proxy-management-tools">3. Proxy Management Tools</a> </li> </ul> </li> <li class="toc-section"> <a href="#patterns-and-best-practices-for-javascript-web-scr">Patterns and Best Practices for JavaScript Web Scraping</a> <ul class="toc-subsections"> <li class="toc-subsection"> <a href="#1-handling-asynchronous-requests">1. Handling Asynchronous Requests</a> </li> <li class="toc-subsection"> <a href="#2-rotating-proxies-regularly">2. Rotating Proxies Regularly</a> </li> <li class="toc-subsection"> <a href="#3-using-headless-browsers">3. Using Headless Browsers</a> </li> </ul> </li> <li class="toc-section"> <a href="#handling-dynamic-content">Handling Dynamic Content</a> <ul class="toc-subsections"> <li class="toc-subsection"> <a href="#example-using-apify-to-handle-dynamic-content">Example: Using Apify to Handle Dynamic Content</a> </li> <li class="toc-subsection"> <a href="#ensuring-compliance-with-gdpr-and-other-data-prote">Ensuring Compliance with GDPR and Other Data Protection Regulations</a> </li> <li class="toc-subsection"> <a href="#example-using-scrapedo-to-handle-proxies-and-captc">Example: Using Scrape.do to Handle Proxies and CAPTCHAs</a> </li> <li class="toc-subsection"> <a href="#legal-aspects-of-web-scraping">Legal Aspects of Web Scraping</a> </li> <li class="toc-subsection"> <a href="#example-understanding-website-terms-of-service">Example: Understanding Website Terms of Service</a> </li> <li class="toc-subsection"> <a href="#ensuring-compliance-with-gdpr-and-other-data-prote">Ensuring Compliance with GDPR and Other Data Protection Regulations</a> </li> <li class="toc-subsection"> <a href="#example-using-scrapedo-to-handle-proxies-and-captc">Example: Using Scrape.do to Handle Proxies and CAPTCHAs</a> </li> <li class="toc-subsection"> <a href="#handling-anti-scraping-measures">Handling Anti-Scraping Measures</a> </li> <li class="toc-subsection"> <a href="#example-using-2captcha-to-handle-captchas">Example: Using 2CAPTCHA to Handle CAPTCHAs</a> </li> <li class="toc-subsection"> <a href="#ensuring-compliance-with-website-terms-of-service">Ensuring Compliance with Website Terms of Service</a> </li> <li class="toc-subsection"> <a href="#example-understanding-website-terms-of-service">Example: Understanding Website Terms of Service</a> </li> <li class="toc-subsection"> <a href="#handling-rate-limiting">Handling Rate Limiting</a> </li> <li class="toc-subsection"> <a href="#example-using-scrapedo-to-handle-rate-limiting">Example: Using Scrape.do to Handle Rate Limiting</a> </li> <li class="toc-subsection"> <a href="#ensuring-compliance-with-website-terms-of-service">Ensuring Compliance with Website Terms of Service</a> </li> <li class="toc-subsection"> <a href="#example-understanding-website-terms-of-service">Example: Understanding Website Terms of Service</a> </li> <li class="toc-subsection"> <a href="#handling-anti-scraping-measures">Handling Anti-Scraping Measures</a> </li> <li class="toc-subsection"> <a href="#example-using-2captcha-to-handle-captchas">Example: Using 2CAPTCHA to Handle CAPTCHAs</a> </li> </ul> </li> </ul> </div> <h1 id="overview-of-javascript-for-web-scraping-profession"> Overview of JavaScript for Web Scraping Professionals </h1> <p> As a web scraping professional, understanding the intricacies of JavaScript is crucial for extracting data from dynamic websites. This section provides an overview of the key challenges professionals face and proven solutions for tackling them. </p> <h2 id="key-challenges-in-javascript-web-scraping"> Key Challenges in JavaScript Web Scraping </h2> <h3 id="1-handling-dynamic-content">1. Handling Dynamic Content</h3> <p> JavaScript-heavy websites often employ techniques like AJAX requests to load content dynamically. This makes it challenging for web scrapers to extract data using traditional methods. </p> <h3 id="2-captcha-solving">2. Captcha Solving</h3> <p> Many websites use CAPTCHAs to prevent automated scripts from accessing their content. Solving CAPTCHAs requires sophisticated algorithms and techniques. </p> <h3 id="3-proxies-and-ip-rotation">3. Proxies and IP Rotation</h3> <p> To avoid being blocked by websites, web scrapers need to rotate proxies and manage IP addresses effectively. </p> <h2 id="proven-solutions-for-javascript-web-scraping"> Proven Solutions for JavaScript Web Scraping </h2> <h3 id="1-using-javascript-libraries-and-frameworks"> 1. Using JavaScript Libraries and Frameworks </h3> <p>Libraries like Puppeteer and Playwright provide a high-level interface for interacting with web pages and extracting data.</p> <h3 id="2-captcha-solving-services">2. Captcha Solving Services</h3> <p> Services like Scrape.do, Apify, and 2captcha offer solutions for solving CAPTCHAs and accessing dynamic content. </p> <h3 id="3-proxy-management-tools">3. Proxy Management Tools</h3> <p> Tools like Scrapy-Proxy and Proxys allow web scrapers to manage proxies and rotate IP addresses efficiently. </p> <h2 id="patterns-and-best-practices-for-javascript-web-scr"> Patterns and Best Practices for JavaScript Web Scraping </h2> <h3 id="1-handling-asynchronous-requests"> 1. Handling Asynchronous Requests </h3> <p> When handling asynchronous requests, ensure that you wait for the response before proceeding with further actions. </p> <h3 id="2-rotating-proxies-regularly"> 2. Rotating Proxies Regularly </h3> <p>Rotate proxies regularly to avoid being blocked by websites.</p> <h3 id="3-using-headless-browsers">3. Using Headless Browsers</h3> <p> Using headless browsers like Puppeteer or Playwright can simplify web scraping tasks and improve performance. </p> <p> By understanding these key challenges, proven solutions, and patterns, you can develop effective JavaScript-based web scraping strategies for extracting data from dynamic websites. </p> <h1 id="key-challenges-in-javascript-web-scraping"> Key Challenges in JavaScript Web Scraping </h1> <h2 id="handling-dynamic-content">Handling Dynamic Content</h2> <p> JavaScript-heavy websites often employ techniques like AJAX requests to load content dynamically. This makes it challenging for web scraping professionals to extract data from these sites. </p> <h3 id="example-using-apify-to-handle-dynamic-content"> Example: Using Apify to Handle Dynamic Content </h3> <h3 id="ensuring-compliance-with-gdpr-and-other-data-prote"> Ensuring Compliance with GDPR and Other Data Protection Regulations </h3> <p> Web scraping professionals must ensure that they comply with data protection regulations, such as the General Data Protection Regulation (GDPR) in Europe. </p> <h3 id="example-using-scrapedo-to-handle-proxies-and-captc"> Example: Using Scrape.do to Handle Proxies and CAPTCHAs </h3> <h3 id="legal-aspects-of-web-scraping"> Legal Aspects of Web Scraping </h3> <p> Web scraping professionals must be aware of the legal aspects of web scraping, including obtaining consent from website owners and respecting their terms of service. </p> <h3 id="example-understanding-website-terms-of-service"> Example: Understanding Website Terms of Service </h3> <h3 id="ensuring-compliance-with-gdpr-and-other-data-prote"> Ensuring Compliance with GDPR and Other Data Protection Regulations </h3> <h3 id="example-using-scrapedo-to-handle-proxies-and-captc"> Example: Using Scrape.do to Handle Proxies and CAPTCHAs </h3> <h3 id="handling-anti-scraping-measures"> Handling Anti-Scraping Measures </h3> <p> Web scraping professionals must be aware of anti-scraping measures, such as CAPTCHAs and rate limiting. </p> <h3 id="example-using-2captcha-to-handle-captchas"> Example: Using 2CAPTCHA to Handle CAPTCHAs </h3> <h3 id="ensuring-compliance-with-website-terms-of-service"> Ensuring Compliance with Website Terms of Service </h3> <p>Web scraping professionals must ensure that they comply with website terms of service, including obtaining consent from website owners.</p> <h3 id="example-understanding-website-terms-of-service"> Example: Understanding Website Terms of Service </h3> <h3 id="handling-rate-limiting">Handling Rate Limiting</h3> <p>Web scraping professionals must be aware of rate limiting, which can prevent them from scraping a website too frequently.</p> <h3 id="example-using-scrapedo-to-handle-rate-limiting"> Example: Using Scrape.do to Handle Rate Limiting </h3> <h3 id="ensuring-compliance-with-website-terms-of-service"> Ensuring Compliance with Website Terms of Service </h3> <h3 id="example-understanding-website-terms-of-service"> Example: Understanding Website Terms of Service </h3> <h3 id="handling-anti-scraping-measures"> Handling Anti-Scraping Measures </h3> <h3 id="example-using-2captcha-to-handle-captchas"> Example: Using 2CAPTCHA to Handle CAPTCHAs </h3> <p>// Define the function async function solveCaptcha(image</p> <h1 id="proven-solutions-for-javascript-web-scraping-profe"> Proven Solutions for JavaScript Web Scraping Professionals </h1> <h2 id="handling-dynamic-content">Handling Dynamic Content</h2> <h3 id="solution-use-a-headless-browser"> Solution: Use a Headless Browser </h3> <p> One proven solution is to use a headless browser like Puppeteer or Selenium, which allows you to automate the rendering of JavaScript-heavy pages and extract data programmatically. </p> <pre><code class="language-javascript">(async () =&gt; { const browser = await puppeteer.launch(); const page = await browser.newPage(); await page.goto('https://example.com/dynamic-page'); const data = await page.content(); console.log(data); })();</code></pre> <h3 id="legal-aspects-of-web-scraping"> Legal Aspects of Web Scraping </h3> <p> Web scraping professionals must ensure compliance with GDPR and other data protection regulations. </p> <h3 id="solution-implement-data-protection-measures"> Solution: Implement Data Protection Measures </h3> <p> To comply with GDPR, web scraping professionals can implement measures like: </p> <ul> <li> <strong>Obtaining explicit consent</strong>: Ensure that users provide explicit consent for their data to be scraped. </li> <li> <strong>Using anonymization techniques</strong>: Anonymize personal data to protect user identities. </li> <li> <strong>Implementing data retention policies</strong>: Establish clear data retention policies to ensure compliance with regulations. </li> </ul> <h2 id="proxies-captchas-and-ip-rotation"> Proxies, CAPTCHAs, and IP Rotation </h2> <p> Web scraping professionals often face challenges with proxy services, CAPTCHA solving, and IP rotation. </p> <h3 id="solution-use-a-reliable-proxy-service"> Solution: Use a Reliable Proxy Service </h3> <p> One proven solution is to use a reliable proxy service like Scrape.do or Apify, which provides fast, scalable, and maintenance-free solutions for JavaScript-heavy websites. </p> <pre><code class="language-javascript">(async () =&gt; { const api = new scrapeDo.Api(); const response = await api.fetchData('https://example.com/dynamic-page'); console.log(response); })();</code></pre> <h3 id="conclusion">Conclusion</h3> <p> Web scraping professionals must be aware of the challenges and solutions outlined in this section to ensure successful data extraction from JavaScript-heavy websites. By implementing measures like obtaining explicit consent, anonymization techniques, and data retention policies, web scraping professionals can comply with regulations and extract valuable data. </p> <h3 id="additional-resources">Additional Resources:</h3> <ul> <li> <a href="https://scrape-do.com/docs">Scrape.do Documentation</a> </li> <li><a href="https://apify.com/docs">Apify Documentation</a></li> </ul> <h1 id="patterns-and-best-practices-for-javascript-web-scr"> Patterns and Best Practices for JavaScript Web Scraping </h1> <p> As a web scraping professional, understanding the intricacies of JavaScript is crucial for extracting data from dynamic websites. This section provides actionable guidance on patterns and best practices for tackling common challenges in JavaScript web scraping. </p> <h2 id="handling-dynamic-content">Handling Dynamic Content</h2> <p> JavaScript-heavy websites often employ techniques like AJAX requests to load content dynamically. To overcome this challenge: </p> <h3 id="1-use-a-headless-browser">1. Use a Headless Browser</h3> <p>Utilize headless browsers like Puppeteer or Selenium to render the webpage and extract data from dynamic elements.</p> <p>Example:</p> <pre><code class="language-javascript">(async () =&gt; { const browser = await puppeteer.launch(); const page = await browser.newPage(); await page.goto('https://example.com'); const title = await page.title(); console.log(title); })();</code></pre> <h3 id="proxies-and-captchas">Proxies and CAPTCHAs</h3> <p> To avoid being blocked by websites, use proxies and CAPTCHA-solving services like Scrape.do or 2captcha. </p> <pre><code class="language-javascript">(async () =&gt; { const apikey = 'YOUR_API_KEY'; const url = 'https://example.com'; const response = await scrapeDo.get(url, apikey); console.log(response.data); })();</code></pre> <h3 id="email-and-phone-verification"> Email and Phone Verification </h3> <p> To verify user input, use email verification services like Mailgun or phone number verification APIs like Twilio. </p> <pre><code class="language-javascript">(async () =&gt; { const apiKey = 'YOUR_API_KEY'; const apiSecret = 'YOUR_API_SECRET'; const fromEmail = 'your-email@example.com'; const toEmail = 'recipient-email@example.com'; const subject = 'Verification Email'; const body = 'Verify your email address'; await mailgun.messages().send({ from: fromEmail, to: toEmail, subject: subject, text: body }); })();</code></pre> <h3 id="browsers-and-curl">Browsers and Curl</h3> <p> For simple tasks, use curl or a browser's developer tools to extract data. </p> <pre><code class="language-bash">curl -X GET 'https://example.com'</code></pre> <h3 id="infrastructure-and-attack-vectors"> Infrastructure and Attack Vectors </h3> <p>To protect your infrastructure from attack vectors:</p> <h3 id="1-use-aws-or-other-cloud-providers"> 1. Use AWS or Other Cloud Providers </h3> <p> Utilize cloud providers like AWS for scalable and secure infrastructure. </p> <pre><code class="language-javascript">(async () =&gt; { const s3 = new aws.S3({ region: 'your-region', accessKeyId: 'YOUR_ACCESS_KEY_ID', secretAccessKey: 'YOUR_SECRET_ACCESS_KEY' }); await s3.putObject({ Bucket: 'your-bucket-name', Key: 'your-object-key', Body: 'your-object-body' }); })();</code></pre> <h3 id="2-implement-anti-scraping-measures"> 2. Implement Anti-Scraping Measures </h3> <p> Use anti-scraping measures like IP rotation, CAPTCHA solving, and rate limiting to prevent abuse. </p> <pre><code class="language-javascript">(async () =&gt; { const ipRotateInstance = new ipRotate(); await ipRotateInstance.rotate(); // Use the rotated IP address for scraping })();</code></pre> <p> By following these patterns and best practices, you can improve your JavaScript web scraping workflow and avoid common pitfalls. </p> <h2 id="helpful-code-examples">Helpful Code Examples</h2> <pre><code class="language-javascript">// Import the Axios library const axios = require('axios'); // Set up a proxy server (optional) const proxy = 'http://localhost:3000'; // Define the URL of the website you want to scrape const url = 'https://www.example.com'; // Use Axios to make an HTTP GET request to the URL axios.get(url, { proxy }).then(response =&gt; { // Log the response data to the console console.log(response.data); }).catch(error =&gt; { // Log any errors to the console console.error(error); }); // Import the Puppeteer library const puppeteer = require('puppeteer'); // Launch a new browser instance (async () =&gt; { const browser = await puppeteer.launch(); const page = await browser.newPage(); // Navigate to the website you want to scrape await page.goto('https://www.example.com'); // Wait for the page to load await page.waitForSelector('#content'); // Extract data from the page using CSS selectors or DOM manipulation const title = await page.$eval('#title', el =&gt; el.textContent); console.log(title); // Close the browser instance await browser.close(); })(); // Import the Scrape.do library const scrape = require('scrape'); // Define the URL of the website you want to scrape const url = 'https://www.example.com'; // Use Scrape.do to extract data from the webpage scrape(url).then(data =&gt; { // Log the extracted data to the console console.log(data); }).catch(error =&gt; { // Log any errors to the console console.error(error); });</code></pre> <h3 id="key-insights">Key Insights</h3> <p> <strong>Understanding JavaScript for Web Scraping: A Comprehensive Guide</strong> </p> <p> As a web scraping professional, it's essential to grasp the intricacies of JavaScript to extract data from dynamic websites. But what exactly is JavaScript, and how does it impact your work? Simply put, JavaScript is a programming language used to add interactivity to web pages. It allows developers to create dynamic content, respond to user interactions, and update the page without requiring a full page reload. This means that traditional web scraping methods may not be effective against websites that use JavaScript-heavy techniques. </p> <p><strong>The Importance of Proxies and IP Rotation</strong></p> <p> When working with JavaScript-heavy websites, it's crucial to understand how proxies and IP rotation can help you avoid being blocked or banned. A proxy is an intermediary server that sits between your scraper and the target website, allowing you to mask your IP address and appear as if you're coming from a different location. IP rotation involves regularly changing your proxy IP address to avoid detection. This technique is essential for web scraping professionals who want to ensure their scripts remain effective over time. </p> <p> <strong>Connecting the Dots: Browser Automation and JavaScript</strong> </p> <p> Another critical aspect of web scraping with JavaScript is browser automation. By using tools like Puppeteer or Playwright, you can automate the behavior of a real browser instance, allowing you to interact with web pages in a more human-like way. This enables you to fill out forms, click buttons, and even simulate user interactions, making it easier to extract data from complex websites. By combining JavaScript libraries and frameworks with proxy management tools, you can create powerful and efficient scraping solutions that stay ahead of the competition. </p> <p> Let me know if this meets your requirements or if you'd like me to make any adjustments! </p> <h2 id="related-information">Related Information</h2> <p>RELATED INFORMATION</p> <p> As a web scraping professional, understanding the intricacies of JavaScript is crucial for extracting data from dynamic websites. Here are some related concepts and information that can help you deepen your knowledge: </p> <p><strong>Related Concepts:</strong></p> <ul> <li> <strong>Proxies:</strong> Understanding how to use proxies effectively is essential for avoiding IP blocking and improving scraping efficiency. </li> <li> <strong>CAPTCHA Solving:</strong> Knowledge of CAPTCHA solving techniques and services can help you automate this tedious task. </li> <li> <strong>JavaScript Libraries and Frameworks:</strong> Familiarity with popular libraries like jQuery, React, or Angular can aid in automating web scraping tasks. </li> </ul> <p><strong>Additional Resources:</strong></p> <ul> <li> For proxy management and IP rotation, consider using services like <a href="https://scrape.do/">Scrape.do</a>, <a href="https://apify.com/">Apify</a>, or <a href="https://2captcha.com/">2captcha</a>. </li> <li> For CAPTCHA solving, explore options like <a href="https://2captcha.com/">2captcha</a>, <a href="https://www.deathbycaptcha.com/">DeathByCaptcha</a>, or <a href="https://app.hubspot.com/captchasolver">HubSpot's CAPTCHA Solver</a>. </li> </ul> <p><strong>Common Use Cases:</strong></p> <ul> <li> <strong>E-commerce websites:</strong> Web scraping is often used to extract product information, prices, and reviews from e-commerce platforms like Amazon, eBay, or Walmart. </li> <li> <strong>Social media monitoring:</strong> Scraping social media platforms can help monitor brand mentions, track hashtags, and analyze user engagement. </li> <li> <strong>Market research:</strong> Extracting data from online marketplaces, forums, or review websites can provide valuable insights for market research. </li> </ul> <p><strong>Important Considerations:</strong></p> <ul> <li> <strong>Terms of Service (ToS) compliance:</strong> Always ensure that your web scraping activities comply with the website's ToS and respect users' privacy. </li> <li> <strong>Rate limiting and IP blocking:</strong> Be aware of rate limits and IP blocking mechanisms to avoid being banned or throttled. </li> <li> <strong>Data quality and validation:</strong> Ensure that extracted data is accurate, complete, and validated to maintain its integrity. </li> </ul> <p><strong>Next Steps:</strong></p> <ul> <li> Start exploring popular JavaScript libraries and frameworks like jQuery, React, or Angular to automate web scraping tasks. </li> <li> Research and experiment with different proxy services and CAPTCHA solving solutions to find the best fit for your needs. </li> <li> Familiarize yourself with common web scraping tools like <a href="https://curl.se/">Curl</a>, <a href="https://www.getpostman.com/">Postman</a>, or <a href="https://insomnia.io/">Insomnia</a>. </li> <li> Dive deeper into <a href="https://en.wikipedia.org/wiki/Obfuscation">JavaScript deobfuscation</a> techniques to improve your web scraping skills. </li> </ul> </article> <aside class="sidebar"> <h3 id="source-documents">Source Documents</h3> <ul class="source-list"> <li>advanced-web-scraping-in-java-a-comprehensive-guide</li> <li> advanced-web-scraping-techniques-in-php-a-comprehensive-guide </li> <li>index</li> <li>top-10-web-scraper</li> </ul> <h3 id="external-resources">External Resources</h3> <ul> <li> <strong>Repositories:</strong> <ul> <li> <a href="https://github.com/ChrisRoland/WebScraping-in-Java-Code-Repo" rel="noopener" target="_blank">WebScraping-in-Java-Code-Repo</a> </li> <li> <a href="https://github.com/ChrisRoland/WebScraping-in-Java-Code-Repo](https://github.com/ChrisRoland/WebScraping-in-Java-Code-Repo" rel="noopener" target="_blank">WebScraping-in-Java-Code-Repo</a> </li> </ul> </li> <li> <strong>External Resources:</strong> <ul> <li> <a href="https://www.scrapingdog.com/" rel="noopener" target="_blank">www.scrapingdog.com</a> </li> <li> <a href="https://dashboard.scrape.do/sign-up" rel="noopener" target="_blank">dashboard.scrape.do</a> </li> </ul> </li> </ul> <h3 id="related-concepts">Related Concepts</h3> <p> <a href="../concepts/javascript-libraries.html">JavaScript Libraries</a> </p> </aside> </div> <section class="related-content"> <h2 id="related-content">Related Content</h2> <ul class="related-content-list"> <li><a href="reverse.html">Reverse</a></li> </ul> </section> </main> <footer> <p> Created with ❤️ by <a href="https://github.com/StackedQueries/document-ai" target="_blank">Document AI</a> </p> </footer> <script src="../assets/search.js"></script> <script src="../assets/copy-code.js"></script> </body> </html> 